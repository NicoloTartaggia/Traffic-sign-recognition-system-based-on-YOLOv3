\section{Experiments 30\%}
Discuss the experiments that you performed. The exact experiments will vary depending on the project, but you might compare with prior work, perform an ablation study to determine the impact of various components of your system, experiment with different hyperparameters or architectural choices. You should include graphs, tables, or other figures to illustrate your experimental results.
%------------------------------------------------------------------------

\begin{table}
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			Model & Accuracy & Train time (h) \\
			\hline\hline
			YOLO & 0.97 & 24 \\
			tiny-YOLO & 0.84 & 4 \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Metrics for detection model}
	\label{yolo-metrics}
\end{table}

\begin{table}
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			Model & Accuracy & Train time (m) \\
			\hline\hline
			CNN1 & 0.86 & 7 \\
			CNN2 & 0.95 & 41 \\
			CNN3 & 0.96 & 23 \\
			LeNet & 0.93 & 12 \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Metrics for classification models}
	\label{class-metrics}
\end{table}

Initially we trained the models and tested them independently using the validation sets for the classifiers and for the detectors. After that, we proceeded to manually testing the 2 step detection and classification technique, analyzing both images and videos with different scenarios. 


\subsection{Detection models}
We trained the detector models on Google Colab with 2 CPUs @ 2.2 GHz, 13 GB of RAM and Nvidia Tesla K80 GPU. The YOLO model was trained for 4000 epochs with 64 batch size and it took about 24 hours and the tiny-YOLO model was trained for 4000 epoch with 64 batch size and it took about 4 hours.
For training  the models we used convolutional weights that are pre-trained on ImageNet model\footnote{\url{https://pjreddie.com/darknet/imagenet}}. Pre-trained weights was used as initializers for the new training procedures. This technique is also called transfer learning and avoids learning everything from scratch. The dataset used to compute this metric was the validation set manually built, as explained above. The computation of the accuracy value for a detection task is not trivial. We developed a specific method for this purpose. The main idea is that the detector select with bounding boxes the position of the traffic signs, selecting the x and y coordinates of the top left corner, the width and the height of each sign, hence using the YOLO format as for the annotations which represented the ground-truth. So, we compared for each traffic signs one by one these values with 10\% of confidence: this value has been chosen considering the human error made during the dataset construction and a small prediction error. 
The results (see Table \ref{yolo-metrics}) show up a substantial difference between the accuracy values but also a substantial difference on the train times. This is due to the fact that tiny-YOLO is a lighter version of the YOLO model. In any case, considering a detection as a bad one doesn't mean that the detection is wrong, but only not well defined for that sign and consequently implies a higher probability of error for the classifier.


\subsection{Classification models}
We trained the classifier models with 4 CPUs Intel Core i7-6700HQ @ 2.6 GHz, 8 GB of RAM and Nvidia GeForce GTX 950M. 16 batch size and 100 epochs for each model. All models were trained for 100 epochs with 16 batch size and they took different train time. The accuracy value of each models was computed after their training and it used the validation set, as explained above. This metric computes the percentage of correct predictions. The results (see Table \ref{class-metrics}) show up that CNN1 has obtained the worst accuracy value because of its shallowness. The other models reached good and similar accuracy values.


\subsection{Detection with classification testing}
Analyzing the obtained results we selected the best combination of detector and classifier. As detector we had chosen the YOLO model because the detection task is fundamental for a good classification: in fact, for getting accurate predictions the bounding boxes have to be precise as possible. As classifier we had chosen the CNN2 model because it reached the best accuracy value with a reasonable train time. Testing automatically the fully working application is a very difficult and for this reason we spent a lot of time for testing it manually. Initially we tested the 2 step detection and classification for road images analysis and then we tested it for videos analysis. During the video analysis, in fact, each frame is processed independently like a single image. The dataset we used for the experiments is our custom made dataset, as above explained. We also compared the same road in different light conditions, like day and night. During the day, the detector is able to spot traffic signs from a great distance and hence to classify them. During the night, instead, signs can be spotted only at close range: the detection distance can be increased if the road is provided by street lamps which light up the signs enough for their identification. 
The difficulty of this task is the variety of different scenarios that we have to face on road. According with Serna et al. \cite{gamezPaper} results, we found that most of the misclassified traffic signs show the following characteristics:
\begin{itemize}
	\item Bad lightning;
	\item Strong motion blur;
	\item Human added artifacts;
	\item Poor image quality;
	\item Strong shadows or highlights;
	\item Occlusions;
	\item Strong similarity with other signs.
\end{itemize} 
All of the elements listed above are commonly found on the roads and all of them contribute to the increase of the probability of error.